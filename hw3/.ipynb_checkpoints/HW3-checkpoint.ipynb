{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we will implement deep generative models. This assignments mainly consists of two parts:\n",
    "\n",
    "  1. Variational Autoencoder (VAE): \n",
    "  \n",
    "      In the VAE section, you will implement a vanilla VAE and a $\\beta$-VAE. After training, you need to compute the neg-loglikelihood (NLL) using the importance sampling method. Then, you need to investigate how well the representations of VAE and $\\beta$-VAE are disentangled.\n",
    "  \n",
    "  2. Generative Adversarial Network (GAN):\n",
    "  \n",
    "      In the GAN section, you will implement a DCGAN and an LSGAN.\n",
    "\n",
    "Submission requirements: you need to submit both of the following files.\n",
    "  - hw3.ipynb: finish all the requirements and submit this Jupyter file.\n",
    "  - report.pdf: See report.ipynb for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.distributions import Normal, kl_divergence, Bernoulli\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidance 0: Dataset\n",
    "\n",
    "We will use a subset of the [dSprites](https://github.com/deepmind/dsprites-dataset) dataset for the VAE models. dSprites is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite. Please run the following cells to make the dataset ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into './dataset/dsprites-dataset'...\n",
      "remote: Enumerating objects: 16, done.\u001b[K\n",
      "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\u001b[K\n",
      "Unpacking objects: 100% (16/16), done.\n"
     ]
    }
   ],
   "source": [
    "# download dataset and prepare dataset directories.\n",
    "dir_name=\"./dataset/dsprites-dataset\"\n",
    "!git clone https://github.com/deepmind/dsprites-dataset.git $dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DspritesDataset(Dataset):\n",
    "  def __init__(self, data_tensor):\n",
    "    super(DspritesDataset, self).__init__()\n",
    "    self.data = data_tensor\n",
    "  \n",
    "  def __len__(self):\n",
    "    return self.data.shape[0]\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('./dataset', 'dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "data = np.load(root, encoding='bytes')\n",
    "imgs = torch.from_numpy(data['imgs']).unsqueeze(1).float()\n",
    "\n",
    "# dsprites has 6 latent factors, controlling the color, shape, scale, rotation and position of the \n",
    "# object. We skip the factor 'rotation' to reduce the dataset size (only take rotation with 0 degrees). \n",
    "imgs = imgs.reshape(3, 6, 40, 32, 32, 1, 64, 64).permute(2, 0, 1, 3, 4, 5, 6, 7)[0]\n",
    "imgs = imgs.reshape(-1, 1, 64, 64)\n",
    "\n",
    "# randomly split the dataset into training set and test set\n",
    "num_data = imgs.shape[0]\n",
    "idx = np.arange(num_data)\n",
    "np.random.shuffle(idx)\n",
    "num_train = int(num_data * 0.8)\n",
    "train_data = imgs[idx[:num_train]]\n",
    "test_data = imgs[idx[num_train:]]\n",
    "\n",
    "train_ds = DspritesDataset(train_data)\n",
    "test_ds = DspritesDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Variational Autoencoder (VAE)\n",
    "\n",
    "VAE is a probabilistic generative model which allows to estimate the data distribution, sample data points, and learn representations. The training process of VAE can be described as follows:\n",
    "\n",
    "- Inference Process:\n",
    "    - Sample $\\mathrm{x}$ from the training set.\n",
    "    - Infer the approximate posterior with encoder network: $q_{\\phi}(\\mathrm{z} | \\mathrm{x}) = \\mathcal{N}(\\mathrm{z} \\mid \\mu_{\\phi}(\\mathrm{x}), \\Sigma_{\\phi}(\\mathrm{x}))$\n",
    "\n",
    "\n",
    "- Generation Process:\n",
    "    - Sample latent variable from the approximate posterior\n",
    "    - Recover reconstruction $\\hat{\\mathrm{x}}$ with decoder network: $p_{\\theta}(\\mathrm{x} ｜ \\mathrm{z}) = \\mathcal{N}(\\mathrm{x} \\mid \\mu_{\\theta}(\\mathrm{z}), \\sigma)$\n",
    "                    \n",
    "                    \n",
    "- Estimate ELBO and maximize ELBO with gradient ascent:\n",
    "    - ELBO = $\\mathbb{E}_{q_{\\phi}(\\mathrm{z} \\mid \\mathrm{x})}[\\log p_{\\theta}(\\mathrm{x} \\mid \\mathrm{z})] - KL\\big(q_{\\phi}(\\mathrm{z} \\mid \\mathrm{x}) \\parallel p(\\mathrm{z})\\big)$\n",
    "    \n",
    "To evaluate a VAE model, we need to estimate the negative log-likelihood (NLL), $- \\log p(x)$, using importance sampling method:\n",
    "\\begin{align*}\n",
    "-\\log p(x) \\approx - \\big[\\log \\frac{1}{k}\\sum^k_{i=1}\\frac{p(\\mathrm{x}, \\mathrm{z}_i)}{q(\\mathrm{z}_i \\mid \\mathrm{x})}\\big]\n",
    "\\end{align*}\n",
    "where $z_i \\sim q(\\mathrm{z}_i \\mid \\mathrm{x})$ and $k$ the number of samples (set $k=50$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 Vanilla VAE\n",
    "\n",
    "**To-dos**:\n",
    "- (5 points) Implement the class Encoder: \n",
    "  - map the input image of size (64, 64) to a latent vector of size (z_dim,)\n",
    "- (5 points) Implement the class Decoder: \n",
    "  - recover the input image from the sampled latent vector\n",
    "- Complete the class VAE:\n",
    "  - (5 points) Complete the **forward()** function \n",
    "  - (5 points) Complete the **comp_nll()** function for model evaluation\n",
    "- (2 points) Generate 16 image samples, plot them in one figure and save the figure\n",
    "- (5 points) Complete the function **traversal_plots()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self, z_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    \"\"\"\n",
    "    You design your own encoder network.\n",
    "    \n",
    "    An example from the Beta-VAE paper is provided below:\n",
    "      Conv 32x4x4 (channel_out 32, kernel 4, stride 2, padding 1),\n",
    "      Conv 32x4x4 (channel_out 32, kernel 4, stride 2, padding 1), \n",
    "      Conv 64x4x4 (channel_out 64, kernel 4, stride 2, padding 1),\n",
    "      Conv 64x4x4 (channel_out 64, kernel 4, stride 2, padding 1),\n",
    "      FC 256 (feature_dim_out 256).\n",
    "      Non-linearity between layers: ReLU activation.\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    ####\n",
    "      \n",
    "  def forward(self, x, testing=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      x: 4D tensor, input images, (B, C, H, W)\n",
    "      testing: boolean, if it's set to True, it means training phase\n",
    "    Outputs:\n",
    "      mu: 2D tensor, mean of the latent distribution\n",
    "      std: 2D tensor, standard deviations of the latent distribution\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    ####\n",
    "    \n",
    "    return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self, z_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "    \"\"\"\n",
    "    You design your own decoder network.\n",
    "    \n",
    "    An example provided from the Beta-VAE paper is to reverse\n",
    "    the encoder defined above using ConvTranspose2d layers.\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    ####\n",
    "    \n",
    "    \n",
    "  def forward(self, z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      z: 2D tensor, latent variable, (B, z_dim)\n",
    "    Output:\n",
    "      rec_mu: 4D tensor, reconstructed images, (B, C, H, W)\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    ####\n",
    "     \n",
    "    return rec_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(nn.Module):\n",
    "  def __init__(self, z_dim=10):\n",
    "    super(VanillaVAE, self).__init__()\n",
    "    self.enc = Encoder(z_dim)\n",
    "    self.dec = Decoder(z_dim)\n",
    "    \n",
    "  def forward(self, x, testing=False, k=50):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      x: 4D tensor, input images, (B, C, H, W)\n",
    "      testing: boolean, if it's set to True, it means training phase\n",
    "      k: number of samples\n",
    "    Output:\n",
    "      loss: scalar, averaged loss over this batch\n",
    "      rec_mu: 4D tensor, reconstructed images, (B, C, H, W)\n",
    "    \"\"\"\n",
    "    if testing:\n",
    "      ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      ###        End of the code    ####\n",
    "      return nll\n",
    "    else:\n",
    "      ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      ###        End of the code    ####\n",
    "    \n",
    "      return loss, rec_mu\n",
    "  \n",
    "  def comp_nll(self, *args):\n",
    "    \"\"\"\n",
    "    Compute the NLL using importance sampling.\n",
    "    Output:\n",
    "      nll: 1D tensor, (B, ), negative marginal log-likelihood\n",
    "    \"\"\"\n",
    "      ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "      ###        End of the code    ####\n",
    "    \n",
    "    return  nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correctness checking snippet for NLL calculation is provided below. It assumes that the function **comp_nll( )** takes the $\\log p(\\mathrm{z})$, the $\\log q(\\mathrm{z} \\mid \\mathrm{x})$, and the $\\log p(\\mathrm{x} \\mid \\mathrm{z})$ as input. Given $3$ datapoints with $5$ samples for each of them, if your imeplementation is correct, you will get the NLL results as follows:\n",
    "\n",
    "```\n",
    "tensor([[-1.0962],\n",
    "        [ 1.2672],\n",
    "        [-0.5142]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "log_p = torch.randn((3, 5), dtype=torch.float)\n",
    "log_q = torch.randn((3, 5), dtype=torch.float)\n",
    "log_p_given_z = torch.randn((3, 5), dtype=torch.float)\n",
    "\n",
    "net = VanillaVAE()\n",
    "nll = net.comp_nll(log_p, log_q, log_p_given_z)\n",
    "print(nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple **train_model()** and a simple **eval_model()** helper function are provided as below. Feel free to modify them upon your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, e):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    model: nn.Module, model to be trained\n",
    "    dataloader: dataset iterator\n",
    "    optimizer: optimizer\n",
    "    e: current epoch, used to calculate the current iteration step\n",
    "  \"\"\"\n",
    "  model.train()\n",
    "  train_loss = 0.\n",
    "  for i, img in enumerate(dataloader):\n",
    "    step = e*len(dataloader) + i\n",
    "    loss, rec = net(img.to(device))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.detach()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "      print('Steps:\\t{}/{},\\tloss:{:.6f}'.format(step, len(dataloader)*epochs, loss))\n",
    "      rec_vis = vutils.make_grid(torch.cat([img[:4].cpu(), rec[:4].detach().cpu()], dim=0), nrow=4, pad_value=1)\n",
    "      plt.imshow(rec_vis.permute(1,2,0))\n",
    "      plt.show()\n",
    "        \n",
    "  train_loss = train_loss / float(len(dataloader))\n",
    "  return net, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, k=50):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "    model: nn.Module, model to be trained\n",
    "    dataloader: iterator, dataset loader\n",
    "    k: importance sampling sample numbers\n",
    "  \"\"\"\n",
    "  nll = 0.\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, img in enumerate(dataloader):\n",
    "      B, C, H, W = img.shape\n",
    "      img = img.unsqueeze(1).expand(-1, k, -1, -1, -1).reshape(B*k, C, H, W)\n",
    "      img = img.to(device)\n",
    "      nll_batch = model(img, testing=True)\n",
    "      nll += nll_batch.mean().detach()\n",
    "    \n",
    "  return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training script is provided as follows. At the end of the training, the loss curve and NLL curve will be plotted and saved.\n",
    "\n",
    "Feel free to modify it, and make sure your model has been trained till fully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "epochs, z_dim, batch_size, lr, optimizer = 100, 10, 32, 1e-4, torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(train_ds, batch_size = batch_size, num_workers=16, shuffle=True, drop_last=True)\n",
    "\n",
    "dl_test = DataLoader(test_ds, batch_size = 4, num_workers=16, shuffle=True, drop_last=True)\n",
    "\n",
    "optimizer = optimizer(net.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device('cuda:0') # set it to torch.device('cpu') if you are using cpus\n",
    "\n",
    "net = VanillaVAE(z_dim)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_list = []\n",
    "train_loss = []\n",
    "for e in range(epochs):\n",
    "  net, loss = train_model(net, dl_train, optimizer, e)\n",
    "  train_loss.append(loss.cpu().item())\n",
    "\n",
    "  nll = eval_model(net, dl_test)\n",
    "  nll = nll / float(len(dataloader))\n",
    "  nll_list.append(nll.cpu().item())\n",
    "  print('Epoch:\\t{}/{},\\tnll:{:.6f}'.format(e, epochs, np.mean(nll.item())))\n",
    "  if (e+1) % 10 == 0:\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, 'r-', np.arange(len(nll_list)), nll_list, 'g-')\n",
    "    plt.show()\n",
    "    \n",
    "plt.plot(np.arange(len(train_loss)), train_loss, 'r-', np.arange(len(nll_list)), nll_list, 'g-')\n",
    "plt.show()\n",
    "plt.savefig('./results/vae_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize images by sampling from the prior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### please put your samples generating codes here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate the disentanglement property of the vanilla VAE, we visualize how the reconstruction changes along with the traversal of latent units.\n",
    "\n",
    "This can be done by the following steps:\n",
    "  - Sample the latent variable from the approximate posterior,\n",
    "  - Change the value of one element by traversing over some range (three standard deviations around the unit Gaussian prior mean) while keeping the remaining latent units fixed,\n",
    "  - Recover images from the modified latent variable.\n",
    "  \n",
    "The skeleton of the function **traversal_plots()** has been provided below, please complete it and plot traversal visualization of every single latent unit with vanilla VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traversal_plots(net, img, z_dim):\n",
    "  assert img.shape[0] == 1, \"only support one image for traversal experiment\"\n",
    "  # infer the approximate posterior\n",
    "  mu, std = net.enc(img) \n",
    "  \n",
    "  # sample from the approximate posterior\n",
    "  z = \n",
    "  \n",
    "  # traverse the latent space\n",
    "  interpolation = torch.linspace(-3.*std, 3.*std, steps=10, device=img.device)\n",
    "  rec_mus = []\n",
    "  for dim in range(z_dim):\n",
    "    for val in interpolation:\n",
    "      ###   modify value of one latent unit\n",
    "      \n",
    "      \n",
    "      ### generate images with modified latentd\n",
    "    \n",
    "  return rec_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(np.arange(len(test_ds))) # randomly take one image from test set\n",
    "img_val = test_ds[idx]\n",
    "tra_imgs = traversal(net, img_val[None, ...].to(device), z_dim)\n",
    "tra_vis = vutils.make_grid(torch.cat(tra_imgs, dim=0).cpu(), nrow=z_dim, pad_value=1)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(tra_vis[0], cmap='gray')\n",
    "plt.savefig('./results/vae_traversal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 Beta-VAE\n",
    "\n",
    "[Beta-VAE](https://openreview.net/references/pdf?id=Sy2fzU9gl) encourages the model to learn disentangled representations by modifying the ELBO:\n",
    "\n",
    "\\begin{align*}\n",
    " \\mathcal{L} &= \\mathbb{E}_{q_{\\phi}(\\mathrm{z} \\mid \\mathrm{x})}[\\log p_{\\theta}(\\mathrm{x} \\mid \\mathrm{z})] - \\beta * KL\\big(q_{\\phi}(\\mathrm{z} \\mid \\mathrm{x}) \\parallel p(\\mathrm{z})\\big)\n",
    "\\end{align*}\n",
    "\n",
    "A disentangled representation can be deﬁned as one where single latent units are sensitive to changes in single generative factors, while being relatively invariant to changes in other factors, [Bengio et al., 2013](https://arxiv.org/pdf/1206.5538.pdf).\n",
    "\n",
    "As an example of visualized comparison on disentanglement learned by vanilla VAE and Beta-VAE shown below, it can be seen that Beta-VAE provides better disentanglement:\n",
    "\n",
    "<img src=\"./imgs/disentangle.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-dos**:\n",
    "\n",
    "- Implement the class BetaVAE, You can reuse the modules you have defined in task 1.1.\n",
    "- Implement the training and evaluation script, plot and save the loss/NLL curve.\n",
    "- (20 points) Find the $\\beta$ values by visual inspection that achieves the best disentanglement performance: it needs to be shown in your traversal plot that all of the generative factors ('shape', 'scale', 'posX', and 'posY') has their corresponding sensitive latent units through the traversal of the latent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generative Adversary Network (GAN)\n",
    "\n",
    "In a GAN, two different networks are built to compete against each other: the generator tries to generate images that can fool the discriminator, and the discriminator tries to classify whether a given image is generated from the generator or sampled from the dataset. The objective function is :\n",
    "\n",
    "\\begin{align*}\n",
    "\\underset{G}{\\text{minimize}}\\ \\underset{D}{\\text{maximize}}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the GAN section, we will use the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28\n",
    "batch_size = 32\n",
    "ds_train = datasets.MNIST('./dataset/MNIST', transform=transforms.ToTensor(), download=True)\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 Deep Convolutional GANs ( DCGAN )\n",
    "\n",
    "**To-dos**:\n",
    "- (5 points) Implement the class Generator.\n",
    "- (5 points) Implement the class Discriminator.\n",
    "- (5 points) Implement the training script, and finish the training process.\n",
    "- (5 points) Plot and save the loss curves, 16 fake image samples generated by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, ):\n",
    "    super(Discriminator, self).__init__()\n",
    "    \"\"\"\n",
    "    You can design your discriminator network. An example is provided as follows:\n",
    "    \n",
    "    Conv2D: 32 Filters, 5x5, Stride 1, padding 0\n",
    "    Leaky ReLU(alpha=0.01)\n",
    "    Max Pool 2x2, Stride 2\n",
    "    Conv2D: 64 Filters, 5x5, Stride 1, padding 0\n",
    "    Leaky ReLU(alpha=0.01)\n",
    "    Max Pool 2x2, Stride 2\n",
    "    Flatten\n",
    "    Fully Connected with output size 4 x 4 x 64\n",
    "    Leaky ReLU(alpha=0.01)\n",
    "    Fully Connected with output size 1\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    #### \n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      x: 4D tensor, (B, C, H, W)\n",
    "    Output: 1D tensor, (B,)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    #### \n",
    "    \n",
    "    return self.enc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self, ):\n",
    "    super(Generator, self).__init__()\n",
    "    \"\"\"\n",
    "    You can design your generator network. An example is provided as follows:\n",
    "    \n",
    "    Fully connected with output size 1024\n",
    "    ReLU\n",
    "    BatchNorm\n",
    "    Fully connected with output size 7 x 7 x 128\n",
    "    ReLU\n",
    "    BatchNorm\n",
    "    Resize into Image Tensor of size 7, 7, 128\n",
    "    Conv2D^T (transpose): 64 filters of 4x4, stride 2\n",
    "    ReLU\n",
    "    BatchNorm\n",
    "    Conv2d^T (transpose): 1 filter of 4x4, stride 2\n",
    "    TanH\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    #### \n",
    "  def forward(self, z):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      z: 2D tensor, latent variable, (B, z_dim)\n",
    "    Output: 4D tensor, fake images, (B, C, H, W)\n",
    "    \"\"\"\n",
    "    ###        Start of the code    ####\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "    ###        End of the code    #### \n",
    "    \n",
    "    return self.enc(z).reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2  Least Squares GAN ( LSGAN )\n",
    "\n",
    "[LSGAN](https://arxiv.org/pdf/1611.04076.pdf) provides a more stable loss function:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Generator loss:} && \\ell_G  &=  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right] \\\\\n",
    "\\text{Discriminator loss:} && \\ell_D &= \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "**To-dos**:\n",
    "- (2 points) Implement the LSGAN, you can build your LSGAN based on the DCGAN implemented in the task 2.1.\n",
    "- Implement the training script, and finish the training process.\n",
    "- (3 points) Plot and save the loss curves, 16 fake image samples generated by the LSGAN generator.\n",
    "\n",
    "\n",
    "*Reminder*: this is the end of homework 3, please do remember attach the results plots into the report.ipynb file, and submit a report.pdf along with this Jupyter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
